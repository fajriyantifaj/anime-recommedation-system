# -*- coding: utf-8 -*-
"""recomendationsystem_anime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy78m6I7ndlzlNjpOQYSq5JRnFPq9S3R

**Get data set from kaggle using API**

---
"""

!pip install opendatasets

!pip install kaggle

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d CooperUnion/anime-recommendations-database

! unzip anime-recommendations-database.zip

"""**Import modules**

---


"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd 

import numpy as np 

import random
from random import randint

import matplotlib.pyplot as plt 
# %matplotlib inline 
import seaborn as sns

"""Get to know the dataset


"""

df_anime = pd.read_csv('anime.csv')
df_anime.head()

df_anime.head()

df_anime.info()

"""Clean null data using method .dropna()"""

df_anime.isnull().sum()

df_anime = df_anime.dropna()
df_anime.isnull().sum()

df_rate = pd.read_csv('rating.csv')
df_rate.head()

df_rate.info()

df_rate.isnull().sum()

df_rate.shape

"""###Clean rating -1"""

df_rate = df_rate.drop(df_rate[df_rate.rating == -1].index)
print (df_rate.head())

df_anime = df_anime.rename(columns={'rating': 'avg_rating'})
df_anime.head()

df_rate = df_rate.rename(columns={'rating': 'user_rating'})
df_rate.head()

df = pd.merge(df_anime, df_rate, on='anime_id')
df.head()

df.shape

df.drop(df[df['user_rating'] < 1].index, inplace=True)
len(df['user_rating'])

df.info()

df.isnull().sum()

df.describe().T

"""#**Exploratory data analysis (EDA)**"""

# avg number of anime rated per user
import statistics
ratings_per_user = df.groupby('user_id')['user_rating'].count()
statistics.mean(ratings_per_user.tolist())

"""###**distribution of ratings per user**


"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
ratings_per_user.hist(bins=20, range=(0,1000))

"""The majority of users have rated less than 200 anime."""

import seaborn as sns
merge_rating = df.dropna(axis = 0, subset = ['name'])
anime_ratingCount = (merge_rating.
     groupby(['name','user_rating']).
     size().
     reset_index(name='totalRatingCount')
    )
anime_ratingCount.head()

rating_with_totalRatingCount = merge_rating.merge(anime_ratingCount, left_on = 'name', right_on = 'name', how = 'left')
rating_with_totalRatingCount.head()

print("Number of types of different anime:", df.type.nunique())
print("Types ", df.type.value_counts())

# Creating a dataframe for rating counts
combine_anime_rating = df.dropna(axis = 0, subset = ['name'])
anime_ratingCount = (combine_anime_rating.
     groupby(by = ['name'])['user_rating'].
     count().
     reset_index().rename(columns = {'rating': 'totalRatingCount'})
    [['name', 'user_rating']]
    )

# Plotting the bar plot for top 10 anime as per rating 
top10_animerating=anime_ratingCount[['name', 'user_rating']].sort_values(by = 'user_rating',ascending = False).head(10)
ax=sns.barplot(x="name", y="user_rating", data=top10_animerating, palette='Blues_r', saturation=1, edgecolor = "#1c1c1c")
ax.set_xticklabels(ax.get_xticklabels(), fontsize=12, rotation=90, ha="right")
ax.set_title('TOP 10 ANIME BASED ON RATING COUNTS',fontsize = 14, weight='bold', pad=20)
ax.set_xlabel('Anime Tittle',fontsize = 14, weight='bold') 
ax.set_ylabel('User Rating count', fontsize = 12, weight='bold')

"""###**TOP 10 ANIME WITH THE HIGHEST MEMBER BASE** """

df.groupby('name').mean().sort_values('members',ascending=False).head(10)['members'].plot(kind='bar',figsize=(12,6)) 
plt.xlabel('Names', fontsize = 12,  weight='bold')
plt.ylabel('Members', fontsize = 12,  weight='bold')
plt.title('TOP 10 ANIME WITH THE HIGHEST MEMBER BASE', weight='bold', pad=20, fontsize = 16)
plt.ticklabel_format(style='plain', axis='y')

"""'Death Note' has the most community members, followed by 'Shingeki no Kyojin' and 'Sword Art Online.'"""

df.type.unique()

print("Number of types of different anime:", df.type.nunique())
print("Types: ", df.type.value_counts())

"""##**MOST VIEWED ANIME TYPES**"""

fig = plt.figure(figsize=(10,10))
sns.countplot(df['type'], palette='deep')
plt.title("Most Viewed Anime", fontsize=20, weight='bold')
plt.xlabel("Types", fontsize=20)
plt.ylabel("Number of Views with Reviews", fontsize = 20)
plt.legend(df['type'])
plt.ticklabel_format(style='plain', axis='y')
plt.show()

sns.histplot(df, x="user_rating")
plt.xlabel("User rating")
plt.ylabel("Count of users")
plt.title("Distribution of User Rating")
plt.ticklabel_format(style='plain') 
plt.show()

"""* Most of the Anime ratings are spread between 6.0 - 9.0
* The mode of the users ratings distribution is around 7.0 - 8.0

#**Modelling**

##KNN 

---

The recommendation system tries to get a rating for an anime in this project. It uses collaborative filtering system algorithm such as
KNNto make an accurate recommendation to the users.
* Collaborative Filtering
Based on past behavior, this approach builds a model of the user. Users may have watched videos, purchased items, or given ratings on items in the past. Using this model, the model can predict the item or a rating for the item that a user might be interested in based on an association between the user and the item.KNN is used as a collaborative filtering approach in recommender systems.
    * KNN
Collaborative filtering systems use the actions of users to recommend other most liked anime to the users. User-based approaches
and item-based approaches can both be used. However, item-based approaches are usually preferred. Users are often more difficult to scale due to their dynamic nature, whereas items usually don't change too much and can be computed offline and served without constantly retraining. KNN can be used to develop a collaborative filtering system based on items in order to make inferences about
new samples; it uses a database in which data points are grouped into several clusters.
"""

anime_pivot=df[:100000].pivot_table(index='name',columns='user_id',values='user_rating').fillna(0)

from scipy.sparse import csr_matrix

matrix = csr_matrix(anime_pivot.values)

from sklearn.neighbors import NearestNeighbors

model_cf = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_cf.fit(matrix)

distances, indices = model_cf.kneighbors(anime_pivot.iloc[11,:].values.reshape(1, -1), n_neighbors = 6)

"""  * Working of KNN
Based on item feature similarity, KNN makes no assumptions about the distribution of the underlying data. As shown below, it
calculates the "distance" between the target most rated anime and every other anime in the database, then ranks their distances and
returns the most liked anime's as recommendations.
"""

for i in range(0, len(distances.flatten())):
    if i == 0:
        print('Recommendations for {0}:\n'.format(anime_pivot.index[11]))
    else:
        print('{0}: {1}, with distance of {2}:'.format(i, anime_pivot.index[indices.flatten()[i]], distances.flatten()[i]))

df.shape

"""##Content Based

---

The content-based filtering uses the features of the items and a userâ€™s profile created
from the data of previous activities to generate recommendations. This technique is
most appropriate when known information about the items (name, area, portrayal). In
content-based recommender system, recommendations are treated as a user-specific
classification problem where the classifier is trained based on the likes and dislikes of
the user and item features. Items are described using keywords, and a client profile is
created to store the kind of items the client likes. These algorithms use this user profile
to recommend items similar to those the client is currently examining or enjoyed in
the past.
"""

df = df.sample(frac=1, random_state=42)
df.head()

df.shape

df_sample = df[:5000]
df_sample.head()

df_sample.shape

df_sample = df_sample.drop_duplicates('anime_id')

"""* TfidfVectorizer initialization
* count idf to genre 
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 

tf = TfidfVectorizer()

tf.fit(df_sample['genre']) 

tf.get_feature_names()

"""* fit transform to matrix"""

tfidf_matrix = tf.fit_transform(df_sample['genre']) 

tfidf_matrix.shape

"""* change vektor tf-idf  into matrix using todense() method


"""

tfidf_matrix.todense()

"""* Make dataframe to tf-idf matrix



"""

pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=df_sample.name
).sample(22, axis=1).sample(10, axis=0)

"""

*   Makes dataframe of variable cosine_sim with rows and columns anime's name
*   count cosine similarity in matrix tf-idf


"""

from sklearn.metrics.pairwise import cosine_similarity

cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

"""* matrix similarity at every anime's name"""

cosine_sim_df = pd.DataFrame(cosine_sim, index=df_sample['name'], columns=df_sample['name'])
print('Shape:', cosine_sim_df.shape)
 

cosine_sim_df.sample(20, axis=1).sample(20, axis=0)

"""* Taking data with the largest index similarity

"""

def anime_recommendations(name, similarity_data=cosine_sim_df, items=df_sample[['name', 'genre']], k=5):
  index = similarity_data.loc[:,name].to_numpy().argpartition(
        range(-1, -k, -1))
    

  closest = similarity_data.columns[index[-1:-(k+2):-1]]


  closest = closest.drop(name, errors='ignore')
 
  return pd.DataFrame(closest).merge(items).head(k)

anime_recommendations('Hunter x Hunter (2011)')